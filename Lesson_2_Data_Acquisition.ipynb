{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TA5tx3vSTfUlXSzTaY7bdaQHE6w3lLdf","timestamp":1688345226245}],"authorship_tag":"ABX9TyPAT6mF6qFmyB3aXINwNQdP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Reading Files into Dataframes\n"," Let's dive into how to use Pandas to load CSV files and perform basic operations on the data.\n","\n","We start by importing the Pandas library using the import pandas as pd statement.\n","\n","To load a CSV file into a DataFrame, we use the pd.read_csv() function and pass the path to the CSV file as the argument. Replace 'path/to/your/file.csv' with the actual file path.\n","\n","If you are using **colab** then\n","\n","1. Click on the Files icon in the left sidebar to use the Using the Files explorer\n","2. Click on the Upload button. Select the lot file from your local machine and click Open\n","3. Copy the file path and place in pd.read_csv\n","\n","After loading the data, we can display the first few rows of the DataFrame using the df.head() method. This provides a quick overview of the data structure and column names."],"metadata":{"id":"F5o3pcCAKxHi"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load a CSV file into a DataFrame\n","\n","# set the path to your path and file name ie. 'path/to/your/file.csv'\n","\n","csv_file = '/content/kaggle-house-price-data-set.csv'\n","\n","df = pd.read_csv(csv_file)\n","\n","# Display the first few rows of the DataFrame\n","print(df.head())\n"],"metadata":{"id":"aVzC2J5PK5Kk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WGET\n","\n","A lot of data can be sourced online. Lets use a function to pull a data file directly from the web.\n","\n","To retrieve the dataset lets use **wget** which is a It is a popular tool for downloading files and does not require any additional libraries to be loaded or installed. Recall we use this file in our data wrangling course so lets get the raw file from the github repository."],"metadata":{"id":"vQdpxl2D8tLX"}},{"cell_type":"code","source":["# WGET with HTTPS file path\n","\n","!wget kaggle-house-price-data-set.csv https://raw.githubusercontent.com/odsc2015/Data-Wrangling-With-SQL/main/kaggle-house-price-data-set.csv\n","\n","# Reanme the retrieved file using -O parameter\n","!wget -O second_house_price_set.csv  https://raw.githubusercontent.com/odsc2015/Data-Wrangling-With-SQL/main/kaggle-house-price-data-set.csv\n","\n","# Load the dataset\n","house_df = pd.read_csv('second_house_price_set.csv')\n","\n","house_df.columns\n"],"metadata":{"id":"UVqaY0xN8r7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Describe Our File\n","\n","\n","Recall We can use the df.describe() method to get summary statistics of the data, such as count, mean, standard deviation, minimum, and maximum values for numeric columns.\n","\n","Accessing specific columns is straightforward.  \n","\n","We can filter the data based on conditions using boolean indexing. In the example, df['Column_Name'] > 100 filters the DataFrame to include only rows where the values in the 'Column_Name' column are greater than 100. The filtered data is stored in the filtered_data variable.\n","\n","Finally, Pandas provides a wide range of functionalities for data manipulation and analysis, allowing you to perform various operations like data transformations, aggregations, merging datasets, and more."],"metadata":{"id":"LMMxskPaLIHw"}},{"cell_type":"code","source":["\n","# Get summary statistics of the data\n","print(house_df.describe(include='all'))\n","\n","# Access specific columns\n","print(house_df['SalePrice'])\n","\n","# Filter data based on conditions\n","filtered_data = house_df[house_df['SalePrice'] > 500000]"],"metadata":{"id":"23qrgFHcLWID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Web Scraping\n","\n","Lets demonstrate the basic process of web scraping using the BeautifulSoup library.\n","\n","We start by sending a GET request to the website of interest using the requests.get() function. In this example, we scrape data from 'https://www.example.com', but you can replace it with the URL of the website you want to scrape.\n","\n","Next, we create a BeautifulSoup object by passing the response content (response.content) and the parser to use (in this case, 'html.parser'). The BeautifulSoup library parses the HTML content and provides methods for extracting specific data.\n","\n","We use the find() method of the BeautifulSoup object to locate specific HTML elements. In this example, we extract the text within the first <h1> tag using soup.find('h1').text. We also extract all paragraphs (<p> tags) on the page using soup.find_all('p').\n","\n","Finally, we print the extracted data to the console."],"metadata":{"id":"sRiNIFPN-3RR"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# Send a GET request to the website\n","url = 'https://www.example.com'\n","response = requests.get(url)\n","\n","# Create a BeautifulSoup object to parse the HTML content\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","# Extract data from specific elements\n","title = soup.find('h1').text\n","paragraphs = soup.find_all('p')\n","\n","# Print the extracted data\n","print(\"Title:\", title)\n","print(\"Paragraphs:\")\n","for p in paragraphs:\n","    print(p.text)"],"metadata":{"id":"uN7jUPoT-4ln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Quick Tip.\n","\n","Most browser allow you to right click and inspect webpage elements. There you can find the HLTM tags you seek!\n","\n","# Web Scraping\n","Lets try something more interesting. Lets scrape Apple's stock price from Google Finance using BeautifulSoup, you'll need to perform several steps:\n","\n","1. Send an HTTP request to Google Finance's Apple stock page.\n","2. Parse the HTML content of the page using BeautifulSoup.\n","3. Locate and extract the relevant stock price element from the parsed HTML.\n","\n","To find the correct element inspect the Apple stock price element at this URL https://www.google.com/finance/quote/AAPL:NASDAQ It should look somethig like this\n","\n","![Stock Price](https://drive.google.com/uc?id=1mp_jWO2BfnZhsfc4Ob7dk83VyhFvGeCP )\n","\n"],"metadata":{"id":"OJweywbFS-os"}},{"cell_type":"markdown","source":["\n","\n"],"metadata":{"id":"zY3OfqPOSkc9"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# Send an HTTP GET request to the Google Finance page for Apple's stock\n","url = \"https://www.google.com/finance/quote/AAPL:NASDAQ\"\n","response = requests.get(url)\n","\n","# Check if the request was successful (status code 200)\n","if response.status_code == 200:\n","    # Parse the HTML content of the page\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Find the element containing the stock price\n","    price_element = soup.find(\"div\", class_=\"YMlKec fxKbKc\")\n","\n","    if price_element:\n","        # Extract and print the stock price\n","        stock_price = price_element.text.strip()\n","        print(f\"Apple's stock price: {stock_price}\")\n","    else:\n","        print(\"Stock price element not found on the page.\")\n","else:\n","    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"],"metadata":{"id":"a4BlDwSdR89D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Results Review.\n","\n","Heres's how we got the stock price\n","\n","1. We import two libraries: requests for making HTTP requests to the website, and BeautifulSoup for parsing HTML content.\n","2. We define the URL of Apple's stock page on Google Finance and send an HTTP GET request to that URL using the requests.get() function. The response is stored in the response variable.\n","3. We check if the HTTP request was successful by verifying that the status code is 200, which indicates a successful response.\n","4. We create a BeautifulSoup object (soup) by parsing the HTML content of the page. The response.text contains the HTML content, and 'html.parser' is the parser used to parse the HTML.\n","5. We use BeautifulSoup's find() method to locate the HTML element that contains the stock price. In this case, we search for a <div> element with the class \"YMlKec fxKbKc\". Please note that this class selector may change over time, and it's essential to inspect the page's source code to find the correct selector.\n","5. If the price_element is found (i.e., not None), we extract the text within the element using price_element.text and remove any leading or trailing whitespace with strip(). Then, we print the stock price.\n","If the price_element is not found, we print a message indicating that the stock price element was not found on the page.\n","\n","\n","REMEMBER - You must respect the privacy and copyright requirements of any website you scrape data from!\n","\n","## APIs\n","\n","API stands for Application Programming Interface. It's a set of rules and protocols that allows different software applications to communicate with each other. APIs are used to request and exchange data between systems, making it possible for your application to interact with external services, retrieve data, and perform various tasks.\n","\n","## APIs in Action\n","We can use an API to retrieve data from on onlive data provider using the  requests library in Python.\n","\n","We start by sending a GET request to the API endpoint of interest using the requests.get() function. Replace 'https://api.example.com/data' with the actual API endpoint URL.\n","\n","We check the status code of the response using response.status_code. A status code of 200 indicates a successful request.\n","\n","If the request is successful, we extract the data from the response using response.json(). This converts the response content (usually in JSON format) into a Python object that we can work with.\n","\n","Once we have the data, we can process and analyze it further based on our specific requirements.\n","\n"],"metadata":{"id":"2_gXO-0PAUHT"}},{"cell_type":"code","source":["import requests\n","\n","# Send a GET request to the API\n","url = \"https://jsonplaceholder.typicode.com/users/1\"  # Random API that doesnt' require and API 'KEY'\n","response = requests.get(url)\n","\n","# Check the status code\n","if response.status_code == 200:\n","    # Extract data from the response\n","    data = response.json()\n","    print(data)\n","\n","    # Process and analyze the data\n","    # ...\n","else:\n","    print(\"Error:\", response.status_code)"],"metadata":{"id":"Nx_I3X_SAUm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercises\n","\n","## Exercise 1\n","\n","Using WGET retrieve the following file from github and examine it contents\n","\n","https://raw.githubusercontent.com/odsc2015/Data-Wrangling-With-SQL/main/new_customers_attempt_a.csv\n","\n","## Exercise 2\n","\n","Using the requests and BeautifulSoup libraries get the stock price for another NASDAQ exchange stock such as NVDA which has the ticker for NVIDIA\n"],"metadata":{"id":"UmtjFHW2W1RZ"}}]}