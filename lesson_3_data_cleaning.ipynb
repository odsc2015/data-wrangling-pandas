{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCzJenQCQNSPpvFAwpImmh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Cleaning\n","\n","Lets write some code to demonstrate how to handle missing values, outliers, duplicates, data inconsistencies, data validation, and data integrity issues in a house price dataset.\n","\n","# Handling Missing Values:\n","\n","Missing values can occur due to various reasons, such as data collection errors or incomplete records. Here's a more detailed explanation of the missing values section:\n","\n"],"metadata":{"id":"8Ij8qhnBHFu_"}},{"cell_type":"markdown","source":[],"metadata":{"id":"yI0Jnxxjg2iw"}},{"cell_type":"code","source":["# Handling Missing Values:\n","\n","# Import pandas library\n","import pandas as pd\n","\n","!wget https://raw.githubusercontent.com/odsc2015/Data-Wrangling-With-SQL/main/kaggle-house-price-data-set.csv\n","\n","\n","# Load the house price dataset\n","house_data = pd.read_csv('kaggle-house-price-data-set.csv')\n","\n","print(house_data['SalePrice'],'\\n')\n"],"metadata":{"id":"3hytGOIBG6Tc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697988794606,"user_tz":240,"elapsed":1991,"user":{"displayName":"Sheamus McGovern","userId":"13415996168155356894"}},"outputId":"954a2bc3-71db-4886-d2b7-44a0fec1a91e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-22 15:33:12--  https://raw.githubusercontent.com/odsc2015/Data-Wrangling-With-SQL/main/kaggle-house-price-data-set.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460676 (450K) [text/plain]\n","Saving to: ‘kaggle-house-price-data-set.csv’\n","\n","kaggle-house-price- 100%[===================>] 449.88K  2.91MB/s    in 0.2s    \n","\n","2023-10-22 15:33:14 (2.91 MB/s) - ‘kaggle-house-price-data-set.csv’ saved [460676/460676]\n","\n","0       208500\n","1       181500\n","2       223500\n","3       140000\n","4       250000\n","         ...  \n","1455    175000\n","1456    210000\n","1457    266500\n","1458    142125\n","1459    147500\n","Name: SalePrice, Length: 1460, dtype: int64 \n","\n"]}]},{"cell_type":"markdown","source":["First load the house price dataset into a pandas DataFrame called house_data stored in a CSV file named 'house_price_dataset.csv'.\n","\n","Use the isnull() function to identify missing values in the dataset. The isnull() function returns a DataFrame of the same shape as house_data, where each cell contains either True if the value is missing or False if the value is not missing. By applying the sum() function to the resulting DataFrame, we can calculate the total number of missing values for each column.\n","\n","The output of missing_values will display the number of missing values for each column in the dataset. This information helps us understand the extent of missing data in the dataset and identify which columns are affected.\n","\n"],"metadata":{"id":"h5Z3cr6vIV4t"}},{"cell_type":"code","source":["# Check for missing values\n","missing_values = house_data.isnull().sum()\n","\n","print(missing_values,'\\n') ## Difficult to tell which columns hae mising values\n","\n","# Find columns with missing values greater than zero\n","columns_with_missing = house_data.columns[house_data.isnull().sum() > 0]\n","\n","# Print the columns with missing values greater than zero\n","for column in columns_with_missing:\n","    print(f\"Column '{column}' has {house_data[column].isnull().sum()} missing value(s).\")\n","\n","print('\\n')\n","\n","## Drop the rows with missing values\n","house_data_clean = house_data.dropna()  # drop rows\n","\n","\n","# Print the same columns with previous missing values\n","for column in columns_with_missing:\n","    print(f\"Now column '{column}' has {house_data_clean[column].isnull().sum()} missing value(s).\")\n","\n","#print('\\n Column list that  exlcudes dropped columns',house_data_clean.columns)"],"metadata":{"id":"d_g4eAxRLRUK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697989236358,"user_tz":240,"elapsed":275,"user":{"displayName":"Sheamus McGovern","userId":"13415996168155356894"}},"outputId":"26ed2d3a-355d-490f-a765-eacd2a08faaf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Id                 0\n","MSSubClass         0\n","MSZoning           0\n","LotFrontage      259\n","LotArea            0\n","                ... \n","MoSold             0\n","YrSold             0\n","SaleType           0\n","SaleCondition      0\n","SalePrice          0\n","Length: 81, dtype: int64 \n","\n","Column 'LotFrontage' has 259 missing value(s).\n","Column 'Alley' has 1369 missing value(s).\n","Column 'MasVnrType' has 8 missing value(s).\n","Column 'MasVnrArea' has 8 missing value(s).\n","Column 'BsmtQual' has 37 missing value(s).\n","Column 'BsmtCond' has 37 missing value(s).\n","Column 'BsmtExposure' has 38 missing value(s).\n","Column 'BsmtFinType1' has 37 missing value(s).\n","Column 'BsmtFinType2' has 38 missing value(s).\n","Column 'Electrical' has 1 missing value(s).\n","Column 'FireplaceQu' has 690 missing value(s).\n","Column 'GarageType' has 81 missing value(s).\n","Column 'GarageYrBlt' has 81 missing value(s).\n","Column 'GarageFinish' has 81 missing value(s).\n","Column 'GarageQual' has 81 missing value(s).\n","Column 'GarageCond' has 81 missing value(s).\n","Column 'PoolQC' has 1453 missing value(s).\n","Column 'Fence' has 1179 missing value(s).\n","Column 'MiscFeature' has 1406 missing value(s).\n","\n","\n","Now Column 'LotFrontage' has 0 missing value(s).\n","Now Column 'Alley' has 0 missing value(s).\n","Now Column 'MasVnrType' has 0 missing value(s).\n","Now Column 'MasVnrArea' has 0 missing value(s).\n","Now Column 'BsmtQual' has 0 missing value(s).\n","Now Column 'BsmtCond' has 0 missing value(s).\n","Now Column 'BsmtExposure' has 0 missing value(s).\n","Now Column 'BsmtFinType1' has 0 missing value(s).\n","Now Column 'BsmtFinType2' has 0 missing value(s).\n","Now Column 'Electrical' has 0 missing value(s).\n","Now Column 'FireplaceQu' has 0 missing value(s).\n","Now Column 'GarageType' has 0 missing value(s).\n","Now Column 'GarageYrBlt' has 0 missing value(s).\n","Now Column 'GarageFinish' has 0 missing value(s).\n","Now Column 'GarageQual' has 0 missing value(s).\n","Now Column 'GarageCond' has 0 missing value(s).\n","Now Column 'PoolQC' has 0 missing value(s).\n","Now Column 'Fence' has 0 missing value(s).\n","Now Column 'MiscFeature' has 0 missing value(s).\n"]}]},{"cell_type":"markdown","source":["In this code snippet, we use the dropna() function to remove rows with missing values from the dataset. By default, dropna() drops any row that contains at least one missing value. The resulting DataFrame, house_data_clean, will only contain rows where all values are present.\n","\n","Alternatively, if you want to impute (fill) the missing values instead of dropping them, you can use techniques like mean imputation, median imputation, or interpolation:\n","\n","python\n"],"metadata":{"id":"znKnLOUMLZXt"}},{"cell_type":"code","source":["# Store columns with missing values before imputation\n","columns_with_missing_before = house_data.columns[house_data.isnull().any()].tolist()\n","\n","# Impute missing values with column means\n","house_data_imputed = house_data.fillna(house_data.mean())\n","\n","# Store columns with missing values after imputation\n","columns_with_missing_after = house_data_imputed.columns[house_data_imputed.isnull().any()].tolist()\n","\n","# Print columns with missing values before and after imputation\n","print(\"Columns with missing values before imputation:\", columns_with_missing_before)\n","print(\"\\nColumns with missing values after imputation:\", columns_with_missing_after,'\\n')\n","\n",""],"metadata":{"id":"9D_ZRd0zLo6Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Which technique?\n","\n","**Mean Imputation**\n","* Replaces missing values with the mean (average) of the non-missing values.\n","* Best suited for data with a symmetrical distribution, without many outliers.\n","* Can alter data variance and correlations, potentially leading to biased estimates.\n","\n","**Median Imputation**\n","* Fills in missing values using the median (the middle value) of the data.\n","* More robust to outliers or non-normal data compared to mean imputation.\n","* Preserves central tendency but might not accurately reflect complex data characteristics.\n","\n","**Interpolation**\n","* Estimates missing values based on patterns or trends between existing neighboring points.\n","* Particularly effective for time-series data or data where sequential measurements have logical consistency.\n","* Requires that the data has an ordered relationship and doesn't work well for non-sequential data.\n","\n","Each method has its assumptions and implications. It's essential to understand your data and the context of your models before deciding on a method for handling missing values. You can perform sensitivity analyses to understand the impact of different imputation methods.\n","\n","## Non-numeric Data\n","\n","If you want to fill missing values in non-numeric (e.g., string) columns, we need to use different strategies, such as filling with a specific value (e.g., \"Unknown\" or \"N/A\") or using the mode (most common value) for categorical data.\n","\n","Lets give this a shot:\n","\n","We still have columns with missing data, why?\n","\n","The fillna method in pandas is primarily designed to fill missing values in numeric columns. When you use fillna(house_data.mean()), it calculates the mean (average) for each numeric column in the DataFrame house_data and then fills missing values in those columns with the corresponding mean value. It doesn't affect non-numeric (e.g., string) columns because it doesn't make sense to calculate a mean for non-numeric data."],"metadata":{"id":"MPFouEFkeRhG"}},{"cell_type":"code","source":["# Impute missing non-numeric data with the most common value (mode)\n","for column in columns_with_missing_after:\n","\n","    most_common_value = house_data[column].mode().values[0]\n","    ## get  the most common value using mode function\n","    print(column, ' : ', most_common_value,'\\n')\n","    house_data_imputed[column].fillna(most_common_value, inplace=True)  # Notice! no DataFrame assignment!\n","\n","# Store columns with missing values after imputation\n","columns_with_missing_after_all = house_data_imputed.columns[house_data_imputed.isnull().any()].tolist()\n","\n","# Print the DataFrame after imputation\n","print(\"\\nColumns with missing values after imputation:\", columns_with_missing_after_all,'\\n')"],"metadata":{"id":"jUe4K1mwc-Bz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the fillna( ) function is used to fill missing values with the mean of each column. You can replace house_data.mean() with other imputation strategies based on the characteristics of your dataset.\n","\n","## Inplace\n","\n","You may have noticed the inplace=True as an argument above and its actually used in  many function calls. It determines whether the operation will be performed on the original DataFrame itself, or if it will return a new DataFrame while leaving the original one unchanged.\n","\n","### Without inplace=True:\n","When inplace is set to False (which is the default setting for most operations), the operation creates a new modified DataFrame and does not affect the original one. You (typically) need to assign the result to a new variable to keep the changes.\n","\n","### With inplace=True:\n","When inplace is set to True, the method directly modifies the original DataFrame. No new DataFrame is created, and nothing is returned by the operation.\n","This approach can be memory-efficient, as you're not creating additional DataFrames, but you need to be certain of the changes you're making as they directly affect your original data.\n","\n","\n","\n","## Dealing with Outliers:\n","\n","Daling with outliers is an important step in data cleaning and analysis. Outliers are data points that significantly deviate from the overall pattern of the dataset."],"metadata":{"id":"nfpu1WxzLxkZ"}},{"cell_type":"code","source":["# Dealing with Outliers:\n","\n","# Import matplotlib.pyplot library for visualization\n","import matplotlib.pyplot as plt\n","\n","# Plot a boxplot of the sales prices\n","plt.boxplot(house_data['SalePrice'])\n","plt.show()\n"],"metadata":{"id":"x8t7rV05ISX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Use the boxplot() function from the matplotlib.pyplot library to visualize the distribution of the 'SalesPrice' column in the house price dataset. A box plot displays the quartiles (25th, 50th, and 75th percentiles), along with any potential outliers. This visualization helps identify the presence of outliers in the data."],"metadata":{"id":"ZooH92nbdBJL"}},{"cell_type":"code","source":["# Remove outliers based on statistical thresholds\n","Q1 = house_data['SalePrice'].quantile(0.25)\n","Q3 = house_data['SalePrice'].quantile(0.75)\n","IQR = Q3 - Q1\n","house_data_clean = house_data[(house_data['SalePrice'] >= Q1 - 1.5 *IQR) & (house_data['SalePrice'] <= Q3 + 1.5*IQR)]\n"],"metadata":{"id":"5qenLlXpdPpR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Lets calculate the Interquartile Range (IQR), which is the difference between the third quartile (Q3) and the first quartile (Q1) and utlize a multiplier of 1.5 as the threshold for detecting outliers.\n","\n","Next, we define the lower and upper bounds for identifying outliers based on the IQR. Data points below the lower bound (Q1 - 1.5 * IQR) or above the upper bound (Q3 + 1.5 * IQR) are considered potential outliers.\n","\n","The code filters the 'SalesPrice' column in the house price dataset using these bounds. By creating a boolean mask (house_data['SalesPrice'] >= Q1 - 1.5*IQR) & (house_data['SalesPrice'] <= Q3 + 1.5*IQR), only the data points within the specified range are selected.\n","\n","The resulting DataFrame, house_data_clean, will exclude the potential outliers based on the statistical thresholds defined by the IQR method.\n","\n"],"metadata":{"id":"dTyMpiyqdYlO"}},{"cell_type":"code","source":["# Plot a boxplot of the sales prices\n","plt.boxplot(house_data_clean['SalePrice'])\n","plt.show()"],"metadata":{"id":"G8wNSnhofxeR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Handling Duplicates\n","\n","Use the duplicated() function from pandas to identify duplicate rows in the house_data DataFrame. By calling house_data.duplicated(), a boolean mask is generated where True indicates a duplicate row and False indicates a unique row. We then use this mask to index the DataFrame and select the duplicate rows.\n","\n","We don't have any duplicate rows in our dataset so we have to create some! To duplicate the first 5 rows of a DataFrame and insert them into the DataFrame, you can use the pd.concat() function to concatenate the DataFrame with itself, taking the first 5 rows.  \n","\n","The resulting DataFrame, duplicate_rows, contains the duplicate records found in the dataset. You can inspect this DataFrame to understand the nature and extent of the duplicates."],"metadata":{"id":"vKpsyGHZf1at"}},{"cell_type":"code","source":["# Print row count before duplication\n","print(\"\\n Row count before duplication:\", len(house_data))\n","\n","# Duplicate the first 5 rows and insert them\n","duplicated_rows = house_data.head(100)\n","house_data = pd.concat([house_data, duplicated_rows], ignore_index=True)\n","\n","# Print row count after duplication\n","print(\"\\n Row count after duplication:\", len(house_data), '\\n')\n","\n","# Find duplicate records based on all columns\n","duplicate_rows = house_data[house_data.duplicated()]\n","\n","# Display the duplicated rows\n","#print(\"Duplicated Rows:\")\n","#print(duplicate_rows)\n","\n","# Drop the duplicate rows\n","house_data = house_data.drop_duplicates()\n","\n","# Print row count after duplication\n","print(\"\\n Row count after dropping duplication:\", len(house_data), '\\n')\n","\n","# Remove duplicates based on specific columns\n","#house_data_clean = house_data.drop_duplicates(subset=['Street', 'Alley', 'LotShape'], keep='first')"],"metadata":{"id":"EVvfb2JHde40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this code snippet, we use the drop_duplicates() function to remove duplicate rows based on specific columns in the house_data DataFrame. The subset parameter specifies the columns to consider when identifying duplicates. In this example, we consider the 'Address', 'Type', and 'Rooms' columns to determine duplicates.\n","\n","By default, drop_duplicates() keeps the first occurrence of each unique row and removes subsequent duplicates. The keep parameter is set to 'first' to retain the first occurrence. You can also choose 'last' to keep the last occurrence or False to remove all duplicates.\n","\n","The resulting DataFrame, house_data_clean, will contain only the unique records after removing duplicates based on the specified columns.\n","\n","#  Addressing Data Inconsistencies:\n","Next lets address inconsistencies in the 'Type' and 'Address' columns of the house_data DataFrame.\n","\n","We can apply string manipulation functions to standardize the values to a desired format. In this example, we use the lower() function to convert all values in the 'Type' column to lowercase, ensuring consistency in case. Similarly, we use the title() function to capitalize the first letter of each word in the 'Address' column, which helps maintain consistent formatting."],"metadata":{"id":"hwWniVo-fPBX"}},{"cell_type":"code","source":["# Addressing Data Inconsistencies:\n","\n","# Lets find the most common value (mode) for each column\n","for column in house_data:\n","    most_common_value = house_data[column].mode().values[0]\n","    ## get  the most common value using mode function\n","    print(column, ' : ', most_common_value,'\\n')\n","\n","\n","# Standardize values using string manipulation functions\n","house_data['BldgType'] = house_data['BldgType'].str.lower()\n","house_data['HouseStyle'] = house_data['HouseStyle'].str.title()\n","\n","print(house_data['BldgType'])\n","\n","# Correct inconsistent values based on domain knowledge\n","house_data.loc[house_data['BedroomAbvGr'] > 100, 'BedroomAbvGr'] = 10"],"metadata":{"id":"oCB4x41ffPiw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Above we also assumed (some abratiralry for this example!) that that the number of rooms in a house should not exceed 10, and any value above that is likely an error.\n","\n","By using the loc indexer, we filter rows where the number of rooms exceeds 10 and replace those values with the maximum acceptable value of 10. This correction ensures that the 'Rooms' column contains consistent and plausible values.\n","\n","we demonstrate how to validate and enforce data types and formats for specific columns in the house_data DataFrame.\n","\n","# Data Validation and Constraints:\n","\n","Lets validate and enforce data types and formats for specific columns in the house_data DataFrame.\n","\n","Use the astype() function to convert the data type of the 'Bedroom' column to an integer (int). This step ensures that the 'Bathroom' column contains whole numbers rather than decimal values.\n","\n","Use the apply() function with a lambda function to format the 'SalesPrice' column. In this example, we apply a format to display the sales prices with two decimal places and commas as thousand separators. This formatting improves the readability and consistency of the 'SalesPrice' values."],"metadata":{"id":"Gx-vKxupCUjh"}},{"cell_type":"code","source":["# Data Validation and Constraints:\n","\n","# Check data types and formats\n","house_data['BedroomAbvGr'] = house_data['BedroomAbvGr'].astype(int)\n","\n","print(house_data['SalePrice'],'\\n')\n","# First Convert 'SalePrice' column to numeric (ignoring non-numeric values)\n","house_data['SalePrice'] = pd.to_numeric(house_data['SalePrice'], errors='coerce')\n","\n","#house_data['SalePrice'] = house_data['SalePrice'].apply(lambda x: '${:,.2f}'.format(x))\n","print('After \\n', house_data['SalePrice'],'\\n')\n","\n"],"metadata":{"id":"0iV1WcMICVCA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["use custom validation rules to ensure that a particular column adheres to specific constraints. In this example, we define a list called valid_types that contains the allowed dwelling types, such as 'house', 'apartment', and 'townhouse'.\n","\n","By using the isin() function, we filter the house_data DataFrame based on the values present in the 'Type' column. Only the rows where the 'Type' value is one of the valid types will be included in the house_data_clean DataFrame. This step ensures that the 'Type' column contains only the predefined valid values.\n","\n","To find the unique values in a specific column, such as the 'BldgType' column of a DataFrame, you can use the unique() method or the value_counts() method in pandas. Here's how to do it:\n","\n","Using unique():\n"],"metadata":{"id":"fwYB6VCxDBg7"}},{"cell_type":"code","source":["unique_bldg_types = house_data['BldgType'].unique()\n","print(unique_bldg_types)\n","\n","# Implement custom validation rules\n","valid_types = ['1Fam', '2fmCon', 'Duplex','Twnhs']  #  lets assume TwnhsE is not a valid\n","house_data_clean = house_data[house_data['BldgType'].isin(valid_types)]\n","\n","print('\\n', house_data_clean['BldgType'].unique())\n","\n","# Using value_counts() to get both unique values and their counts:\n","print('\\n', house_data_clean['BldgType'].value_counts())\n","\n","#Handling Data Integrity Issues:\n","\n"],"metadata":{"id":"tHQq1Xvus4SK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercises\n","\n","## Exercise 1: Handling Missing Values\n","\n","You have a DataFrame house_data that contains a dataset with missing values. Your task is to identify the columns with missing values and impute them using the mean of each column.\n","\n","Here is some starter code"],"metadata":{"id":"jd5TturIg3lr"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the house price dataset\n","house_data = pd.read_csv('https://raw.githubusercontent.com/odsc2015/Data-Wrangling-With-SQL/main/kaggle-house-price-data-set.csv')\n"],"metadata":{"id":"O24yb_lPhNHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Handling Outliers\n","\n","You have a DataFrame house_data that contains a dataset with potential outliers in the 'SalePrice' column. Your task is to identify and remove these outliers based on statistical thresholds.\n","\n","Hint. Use quantile(0.25) and quantile(0.75)\n","\n","## Exercise 3: Handling Duplicates\n","\n","You have a DataFrame house_data that contains a dataset with duplicate rows. Your task is to identify and remove these duplicate rows based on all columns."],"metadata":{"id":"ZEc4RGQ0hDtQ"}},{"cell_type":"markdown","source":["## Exercise 4\n","\n","Replace the missing data with the median, rather than mean value\n","\n","Hint!\n","\n","    house_data_median = house_data.fillna(house_data.median(),inplace=True)\n","\n","## Exercise 5\n","\n","### Interpolation:\n","\n","As we discussed briefly Interpolation estimates the missing values based on patterns or trends between existing neighboring points. For it to work you make an estimate for the missing values based on the relationship between the data. This could be a linear trend, polynomial trend, or other relationships trend around the neighboring data. Thus its Not suitable when there are no clear relationships in the data and could othweise lead to misleading estimations if the trend assumption is wrong.\n","\n","Exercise: Determine for yourself which columsn have a linear relationship and use the following code hint to interpolate that column\n","\n","df['column_name'].interpolate(method='linear', inplace=True)\n"],"metadata":{"id":"fA6V7LqTHZ_d"}}]}